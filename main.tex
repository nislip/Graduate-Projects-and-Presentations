%
% Hello! Here's how this works:
%
% You edit the source code here on the left, and the preview on the
% right shows you the result within a few seconds.
%
% Bookmark this page and share the URL with your co-authors. They
% can edit at the same time!
%
% You can upload figures, bibliographies, custom classes and
% styles using the files menu.
%
% This presentation is made with the Beamer package. For tutorials
% and more info, see:
% http://en.wikipedia.org/wiki/Beamer_(LaTeX)
%
% We're still in beta. Please leave some feedback using the link at
% the top left of this page.
%
% Enjoy!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[8pt]{beamer}

%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{Antibes}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{professionalfonts}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

\usepackage[english]{babel}
\usepackage{listings}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{color, xcolor}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{changepage}


% On writeLaTeX, these lines give you sharper preview images.
% You might want to comment them out before you export, though.
\usepackage{pgfpages}
\pgfpagesuselayout{resize to}[%
  physical paper width=8in, physical paper height=6in]

\title[MATH 585]{Chapter 6: Comparisons of Several Multivariate Means}
\author{Nate Islip}
\institute{Eastern Washington University}
\date{2/12/2020}

\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}


\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
\usepackage{hyperref}

%\usepackage{pdfpages}
%\setbeameroption{show notes on second screen=right}
%\setbeamertemplate{note %page}{\pagecolor{yellow!5}\insertnote}\usepackage{palatino}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN DOCUMENT 

\begin{document}

% TITLE PAGE 

\begin{frame}
  \titlepage
\end{frame}

% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Chapter 6 Motivation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Chapter Objectives}
\begin{itemize}
  \item Extending on ideas in Chapter 5 to handle problems involving the comparison of several mean vectors
  \item Discuss the tenets of good experimental practice (i.e. repeated measure design) and look at the comparison of means
  \item Finally, discuss several comparisons among mean vectors (MANOVA) 
\end{itemize}
    %[item]{This is a test to see if is works, and it works well}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TABLE OF CONTENTS %%%%%%%%%%%%%%%
\frame[allowframebreaks]%
{\frametitle{Table of Contents}\tableofcontents}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%                           6.2 START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{6.2 Paired Comparisons and Repeated Measures Design}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SLIDE 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\label{6.2}
\subsection{6.2 Introduction}

\begin{frame}{6.2: Paired Comparisons} % Title of the current slide/frame 
        \begin{itemize}
            \item Measurements are often recorded under different sets of experimental conditions to see whether the responses differ significantly over these sets. (i.e. Efficacy of drug treatment)
            \item In some cases, we can compare \textbf{two or more} treatments and assess the \textbf{effects of those treatments}. 
            \item One approach would be to assign both treatments to the \textbf{same or identical units}.
            \begin{itemize}
                \item Compute the differences, thereby eliminating much of the influence of extraneous unit-to-unit variation. 
            \end{itemize}
        \end{itemize}
        %[item]{Efficacy of a new drug treatment can be determined by comparing the measurements before the treatment with those after the treatment}
        %[item]{Extraneous variables meaning, variables not included in the experiment }
\end{frame}

% SLIDE 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Paired Comparisons}

    \begin{frame}{6.2: Notation}
    \begin{itemize}
        
        \item Let $X_{j1}$ denote the response to \textit{treatment 1}, and $X_{j2}$ by the response to \textit{treatment 2} for the $j$th trial. Let the $n$ differences be, 
    
      \begin{equation}
        D_{j} = X_{j1} - X_{j2}
      \end{equation}
    
        \item Let $D_{j}$ represent independent observations from an $N(\delta, \sigma_{d}^{2})$ distribution, and define $t$ as,  
        
        \begin{equation}
            t = \frac{\overline{D} - \delta}{s_{d}\sqrt{n}}
        \end{equation}
    
        where,
      
      \begin{align*}
          \overline{D} = \frac{1}{n}\sum_{j=1}^{n}D_{j} && s_{d}^{2} = \frac{1}{n-1}\sum_{j = 1}^{n}(D_{j} - \overline{D})^{2}
      \end{align*}
      
      \item Has a $t$-distribution with $n - 1$ d.f. Consequently an $\alpha$-level test of the Hypothesis ($H_{0}:\delta = 0$ and $H_{1}:\delta \neq 0$) may be conducted. 
     
    \end{itemize}
    %[item]{For equation (1) $j = 1,2,...,n$.}
    %[item]{The $n$ differences should only reflect the differential effects of the treatments}
    %[item]{Note: $H_{0}:\delta = 0$ (zero mean difference for treatments}
    \end{frame}

    \begin{frame}{Notation Cont.}
    
        \begin{itemize}
            \item A $100(1 - \alpha)\%$ \textbf{confidence interval} for the mean difference $\delta = E(X_{j1} - X_{j2})$ is provided as, 
            
            \begin{equation}
                \overline{d} - t_{n-1}(\alpha/2)\frac{s_{d}}{\sqrt{n}} \leq \delta \leq \overline{d} + t_{n-1}(\alpha/2)\frac{s_{d}}{\sqrt{n}}
            \end{equation}
            
            \item For the \textbf{multivariate case} where we denote $p$ responses, two treatments, and $n$ experimental units. 
            
            \begin{align*}
                X_{1jp} = \text{Variable p under treatment 1} \\
                X_{2jp} = \text{variable p under treatment 2}
            \end{align*}
            
        \end{itemize}
        %[item]{For the multivariate extension of the paired comparison procedure. We label $p$ responses within the $j$th unit as}
        %[item]{For Next slide the $p$ paired differences become $D_{jp} = X_{1jp} - X_{2jp}$}
    \end{frame}

    \begin{frame}{Notation Cont.}
    \begin{itemize}
        \item The $p$ paired difference random variables become $D_{jp} = X_{1jp} - X_{2jp}$. Let, $\mathbf{D}'_{j} = [D_{j1},...,D_{jp}]$ and assume for $j = 1,2,...,n$ $E(\mathbf{D}_{j}) = \mathbf{\delta}$ and $Cov(\mathbf{D_{j}}) = \mathbf{\Sigma}_{d}$.
        
        \item If $\mathbf{D_{n}}$ are independent $N_{p}(\mathbf{\delta}, \mathbf{\Sigma}_{d})$ random vectors, inferences about the vector of mean differences $\mathbf{\delta}$ can be based upon a $T^{2}$-Statistic.  
        
        \begin{equation}
            T^{2} = n(\mathbf{\overline{D} - \mathbf{\delta}})'\mathbf{S}_{d}^{-1}(\overline{\mathbf{D}} - \mathbf{\delta})
        \end{equation}
        
        \begin{block}{Result 6.1}
            Let the differences of $\mathbf{D}_{n}$ be a random sample from an $N_{p}(\mathbf{\delta}, \mathbf{\Sigma}_{d})$ population. Then equation (4) is distributed as an $[(n-1)p/(n-p)]F_{p,n-p}$ random variable, whatever the true $\mathbf{\delta}$ and $\mathbf{\Sigma}_{d}$. If n and n - p are both large, $T^{2}$ is approximately distributed as a $\chi^{2}_{p}$ random variable. 
        \end{block}
        
    \end{itemize}
    %[item]{So, for $\mathbf{D}_{j}' = [D_{jp}]$ it is the vector containing all p paired difference random variables}
    %[item]{$\mathbf{S}_{d} = \frac{1}{n - 1}\sum_{j=1}^{n}(\mathbf{D_{j} - \overline{D}})(\mathbf{D}_{j} - \overline{D})'$}
    \end{frame}

    \begin{frame}{Notation Cont.}
        
        \begin{itemize}
            \item A $100(1 - \alpha)\%$ \textbf{confidence region} for $\mathbf{\delta}$ consists of all $\mathbf{\delta}$ s.t.
            
            \begin{equation}
                (\mathbf{\overline{d}} - \mathbf{\delta})'\mathbf{S}^{-1}_{d}(\overline{\mathbf{d}} - \mathbf{\delta}) \leq \frac{(n - 1)p}{n(n-p)}F_{p,n-p}(\alpha)
            \end{equation}
        
            \item \textbf{$100(1 - \alpha)\%$ simultaneous confidence intervals for the individual mean differences} $\delta_{i}$ are given by
            
            \begin{equation}
                \delta_{i}:\overline{d}\pm\sqrt{\frac{(n-1)p}{(n - p)} F_{p,n-p}(\alpha)}\sqrt{\frac{s^{2}_{d_{i}}}{n}}
            \end{equation}
        
            \item \textbf{Bonferroni $100(1 - \alpha)\%$ simultaneous confidence intervals} for the individual mean differences are 
            
            \begin{equation}
                \delta_{i}: \overline{d}_{t} \pm t_{n-1}\left(\frac{\alpha}{2p}\right)\sqrt{\frac{s^{2}_{d_{i}}}{n}}
            \end{equation}
            
            where $t_{n-1}(\alpha/2p)$ is the upper $100(\alpha/2p)$th percentile of a t-distribution with $n-1$ d.f.
        \end{itemize}
        
        
    \end{frame}




%                           6.1 START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Example 6.1}
    \begin{frame}{Example 6.1} 
        \begin{figure}
            \includegraphics[scale=0.33]{example 6.1.PNG}
            \includegraphics[scale=0.33]{Example 6.1 pt 2.PNG}
            \caption{Checking for a mean Difference with paired observations (Effluent Data)}
        \end{figure}
        %[item]{The set up: Concerned about the reliability of the data, so the samples of effluent were divided and sent to two labs.}
        %[item]{for $d_{j1}$ and $d_{j2}$ we are calculating the differences of paired observations. For instance, $6 - 25$, $6 - 28$, and then $27 - 15$, $23 - 13$ so on and so forth. Then we calculate the sample covariance matrix $\mathbf{S}_{d}$ and then using both these components, calculate $T^{2}$}
        %[item]{Next we calculate the F-statistic with degrees of freedom of 2 and 9 at an $\alpha$ of $0.05$ which comes out to be $(2(10)/9)\times 4.26$}
        %[item]{Remember:F statistic also known as F value is used in ANOVA and regression analysis to identify the means between two populations are significantly different or not. In other words F statistic is ratio of two variances (Variance is nothing but measure of dispersion, it tells how far the data is dispersed from the mean)}
    \end{frame}
    
    \begin{frame}{Example 6.1 Conclusions}
            \begin{itemize}
                \item The $T^{2}$ statistic for testing $H_{0}:\mathbf{\delta}' = [\delta_{1},\delta_{2}] = [0,0]$ is constructed from the differences of paired observations.
                \item Since $T^{2} = 13.6 > 9.47$ (Note: taking $\alpha = 0.5$) reject the $H_{0}$ and conclude that there is a non-zero mean difference between the measurements of the two laboratories. 
                \item Compute the 95\% simultaneous confidence intervals for the mean differences $\delta_{1}$ and $\delta_{2}$ can be computed using Equation (6).
                \begin{itemize}
                    \item 95\% \textbf{simultaneous confidence coefficient} applies to the \textit{entire} set of intervals that could be constructed by $a_{1}\delta_{1} + a_{2}\delta_{2}$.
                    \item $\mathbf{\delta} = 0$ falls outside the 95\% \textbf{confidence region for $\mathbf{\delta}$}.
                    \item If $H_{0}: \mathbf{\delta = 0}$ were NOT rejected, then \textit{all} \textbf{simultaneous intervals} would include \textbf{zero}.
                    \item \textbf{Bonferroni simultaneous intervals} also cover zero.
                \end{itemize}
            
            \end{itemize}
            %[item]{Our Analysis assumed a normal distribution for the $\mathbf{D_{j}}$}
            %[item]{CONCLUSION: The evidence points toward \textbf{real differences}. The point $\mathbf{\delta} = 0$ falls outside the $95\%$ confidence region for $\mathbf{\delta}$ and this result is consistent with the $T^{2}$}
    \end{frame}
    

    \begin{frame}{Paired Comparisons Continued}
        \begin{itemize}
            
            \item Concluding paired comparisons by noting that $\mathbf{\overline{d}}$ and $\mathbf{S_{d}}$, and hence $T^{2}$, may be calculated from the full-sample quantities $\mathbf{\overline{x}}$ and $\mathbf{S}$. Where, 
                
            \begin{displaymath}
            \mathbf{S}_{2p \times 2p} = 
            \begin{bmatrix}
                    \mathbf{S}_{11} & \mathbf{S}_{12}\\
                    \mathbf{S}_{21} & \mathbf{S}_{22}\\
            \end{bmatrix}
            \end{displaymath}
        
            \item After defining $\mathbf{S}$ we can define the following matrix $\mathbf{C}$\\
            and verify $\mathbf{d_{j}} = \mathbf{C}\mathbf{x}_{j}$ and $\overline{\mathbf{d}} = \mathbf{C}\mathbf{\overline{x}}$ and $\mathbf{S_{d}} = \mathbf{CSC}'$. Thus, 
            
            \begin{equation} \label{eq:6}
                T^{2} = n\mathbf{\overline{x}'C'(CSC')^{-1}C\overline{x}}
            \end{equation}
            
            Each row of the matrix $\mathbf{C}$ is a \textbf{contrast vector} because its elements sum to zero. Each Contrast is perpendicular to the vector $\mathbf{1}'$ since $\mathbf{c'_{j}1} = 0$ the component $\mathbf{1'x_{j}}$, representing the overall treatment sum, is ignored by $T^{2}$ presented in this section. 
                
        \end{itemize}
        %[item]{$\mathbf{S}$ is the $2p \times 2p$ matrix of sample variances and co variances. The matrix $\mathbf{S_{11}}$ contains the sample variances and co variances for the p variables on treatment 1. $\mathbf{S}_{22}$ contains sample variances and co variances computed for the p variables on treatment 2. }
        %[item]{$\mathbf{S_{12}} = \mathbf{S_{21}'}$ are the matrices of sample co variances computed from observations on pairs of treatment 1 and treatment 2 variables.}
    \end{frame}

\subsection{Repeated Measures Design for Comparing Treatments}
        \begin{frame}{Notation}
            \begin{itemize}
                \item Now we will observe situations where $q$ treatments are compared w.r.t. a single response variable. 
                \begin{itemize}
                    \item Each subject (or experimental unit) receives each treatment once over successive periods of time. 
                \end{itemize}
                
                \item Denote $\mathbf{X}_{j}$ for the $j$th observation where $X_{ji}$ is the response to the $i$th treatments on the $j$th unit. 
            \end{itemize}
            
            \begin{equation*}
                \mathbf{X}_{j} = \begin{bmatrix}
                    X_{j1} \\
                    X_{j2} \\ 
                    \vdots \\
                    X_{jq}
                \end{bmatrix}
            \end{equation*}
            %[item]{Repeated measures stems from the fact that all treatments are administered to each unit.}
        \end{frame}
        
        \begin{frame}
            
            \item Consider the contrasts of the components of $\mathbf{\mu} = E(\mathbf{X_{j}})$ these could be $\mathbf{C_{1}}\mu$ or $\mathbf{C_{2}}\mu$. 
            
            \begin{itemize}
                \item When $\mathbf{C}_{1}\mu = \mathbf{C_{2}}\mu = 0$ the hypothesis that there are no differences in treatments becomes $\mathbf{C}\mu = 0$ for any choice $\mathbf{C}$. 
                \item Test $\mathbf{C\mu} = 0$ using equation (\ref{eq:6})
            \end{itemize}
            
            \begin{displaymath}
                \begin{bmatrix}
                        \mu_{1} - \mu_{2}\\
                        \mu_{1} - \mu_{3} \\
                        \vdots \\
                        \mu_{1} - \mu_{q}
                \end{bmatrix}
                =
                \begin{bmatrix}
                        1 & -1 & 0 &\hdots & 0\\
                        1 & 0 & -1 &\hdots & 0 \\
                        \vdots & \vdots & \vdots & \ddots & \vdots\\
                        1 & 0 & 0 & \hdots & -1
                \end{bmatrix}
                \begin{bmatrix}
                        \mu_{1} \\
                        \mu_{2} \\
                        \vdots \\
                        \mu_{q}
                \end{bmatrix}
                = \mathbf{C}_{1}\mathbf{\mu}
            \end{displaymath}
            
            Or, 
            
            \begin{displaymath}
                \begin{bmatrix}
                        \mu_{2} - \mu_{1}\\
                        \mu_{3} - \mu_{2} \\
                        \vdots \\
                        \mu_{q} - \mu_{q-1}
                \end{bmatrix}
                =
                \begin{bmatrix}
                        -1 & 1 & 0 & \hdots & 0 & 0\\
                        0 & -1 & 1 & \hdots & 0 & 0\\
                        \vdots & \vdots & \vdots & \ddots & \vdots\\
                        1 & 0 & 0 & \hdots & -1 & 1
                \end{bmatrix}
                \begin{bmatrix}
                        \mu_{1} \\
                        \mu_{2} \\
                        \vdots \\
                        \mu_{q}
                \end{bmatrix}
                = \mathbf{C}_{1}\mathbf{\mu}
            \end{displaymath}
            
            %[item]{$\mathbf{C_{1},C_{2}}$ are contrast matrices because their $q-1$ rows are linearly independent and each is a contrast vector. \textbf{The nature of the design eliminates much of the influence of unit to unit variation on treatment comparisons}. The experimenter should randomize the order in which the treatments are presented to each subject.}
        \end{frame}
        
        
\subsection{Test for Equality of Treatments in a Repeated Measures Design}
    \begin{frame}{Test for Equality of Treatments}
        \begin{itemize}
            \item Consider $N_{q}(\mathbf{\mu, \Sigma})$ population and let $\mathbf{C}$ be a contrast matrix. An $\alpha$-level test of $H_{0}:\mathbf{C\mu = 0}$ versus $H_{1}:\mathbf{C\mu \neq 0}$ is as follows: Reject $H_{0}$ if
            
            \begin{equation}
                T^{2} = n(\mathbf{C\overline{x}})'(\mathbf{CSC'})^{-1}\mathbf{C\overline{x}} > \frac{(n - 1)(q - 1)}{(n - q + 1)}F_{q-1,n-q+1}(\alpha)
            \end{equation}
            
            \item A confidence region for contrasts $\mathbf{C\mu}$ with $\mathbf{\mu}$ the mean of a normal population is determined by the set of all $\mathbf{C\mu}$ s.t. 
            
            \begin{equation}
                n(\mathbf{C\overline{x}})'(\mathbf{CSC'})^{-1}\mathbf{C\overline{x}} \leq \frac{(n - 1)(q - 1)}{(n - q + 1)}F_{q-1,n-q+1}(\alpha)
            \end{equation}
            
            \item Simultaneous $100(1-\alpga)$\% confidence intervals for single contrasts $\mathbf{c'\mu}$ for any contrast vectors of interest are given by, 
            
            \begin{equation}
                \mathbf{c'\mu}: \mathbf{c'\overline{x}} \pm \sqrt{\frac{(n - 1)(q - 1)}{(n - q + 1)}F_{q-1,n-q+1}(\alpha)}\sqrt{\frac{\mathbf{c'Sc}}{n}}
            \end{equation}
        \end{itemize}
        %[item]{This is a test to see if is works, and it works well}
    \end{frame}
    
\subsubsection{Example 6.2}
    \begin{frame}{Example 6.2}
        \begin{figure}
            \includegraphics[scale=0.24]{table 6.2 Sleeping dog data.PNG}
            \includegraphics[scale=0.24]{example 6.2.PNG}
            \includegraphics[scale=0.24]{Example 6.2 contrasts.PNG}
            \caption{Sleeping Dog Data \& Computations}
        \end{figure}
        
        \begin{align*}
            \frac{(19 - 1)(4 - 1)}{(19 - 4 + 1)}F_{3,16}(0.5) = 10.94 && \text{From (7) } T^{2} = 116 > 10.94 && \text{Reject } H_{0}:\mathbf{C\mu} = \mathbf{0}
        \end{align*}
        
        %[item]{Sample Code: https://www.uio.no/studier/emner/matnat/math/nedlagte-emner/STK4040/h11/R-lectures-week39.html}
        %[item]{There are three \textbf{treatment contrasts} that might be of interest in the experiment. The $\mu$ correspond to the \textbf{mean responses for treatments 1,2,3, and 4 respectively}}
        %[item]{From (7) we reject the null hypothesis (no treatment effects)}
        %[item]{To see which \textbf{contrast} are responsible for the rejection of $H_{0}$ we construct the \textbf{simultaneous confidence intervals} for these contrasts}
    \end{frame}

%                           6.3 START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{6.3 Comparing Mean Vectors from Two Populations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Notation & Theory}
    \begin{frame}{6.3 Introduction} % Title of the current slide/frame 
        \begin{itemize}
        
        \item This $T^{2}$ statistic is appropriate for comparing responses from one-set of experimental settings (population 1) with the independent responses from another set of experimental settings (population 2)
        
        
       \item Consider a random sample of size $n_{1}$ from population 1 and a sample of size $n_{2}$ from population 2. The observations on $p$ variables are arranged as follows. 
        
        \begin{center}
            \renewcommand{\arraystretch}{2}
            \begin{tabular}{ccc}
            \textbf{Sample} & & \textbf{Summary Statistics}\\
            \hline
                (population 1) & $\mathbf{\overline{x}_{1}} = \frac{1}{n_{1}}\sum_{j=1}^{n_{1}}x_{1j}$ & $\mathbf{S_{1} = \frac{1}{n_{1}-1}\sum_{j=1}^{n_{1}}}(\mathbf{x_{1j}} - \mathbf{\overline{x_{1}}})(\mathbf{x_{1j} - \overline{x}_{1}})'$ \\
                (population 1) & $\mathbf{\overline{x}_{1}} = \frac{1}{n_{2}}\sum_{j=1}^{n_{1}}x_{2j}$ & $\mathbf{S_{1} = \frac{1}{n_{2}-1}\sum_{j=1}^{n_{2}}}(\mathbf{x_{2j}} - \mathbf{\overline{x_{2}}})(\mathbf{x_{2j} - \overline{x}_{2}})'$\\
            \hline\\
            \end{tabular}
        \end{center}
    
        \item Want to answer the question $\mathbf{\mu_{1}} = \mu_{2}$ and if $\mathbf{\mu_{1} - \mathbf{\mu_{2}}} \neq \mathbf{0}$ which component means are different? 
        
        \end{itemize}
        %[item]{This $T^{2}$ for testing equality of vector means from two multivariate populations can be developed by analogy with the univariate case}

        %[item]{The comparison can be made without explicitly controlling for unit-to-unit variability, as in the paired comparisons case}
    \end{frame}
    
\subsection{Assumptions and Common Covariance}
    \begin{frame}{Assumptions and Common Covariance}
    \textbf{Assumptions Concerning the structure of the data:}
        \begin{enumerate}
            \item The sample $\mathbf{X_{1n_{1}}}$ is a random sample size $n_{1}$ from a $p$-variate population with mean vector $\mathbf{\mu_{1}}$ and co variance matrix $\mathbf{\Sigma_{1}}$
            \item The sample $\mathbf{X_{2n_{2}}}$ is a random sample of size $n_{2}$ from a $p$-variate population with mean vector $\mathbf{\mu_{2}}$ and co variance matrix $\mathbf{\Sigma_{2}}$
            \item $\mathbf{X_{11},X_{12},...,X_{1n_{1}}}$ are independent of $\mathbf{X_{21}, X_{22},...,X_{2n_{2}}}$
        \end{enumerate}
    \textbf{Assumptions When $n_{1}$ and $n_{2}$ are small:}
        \begin{enumerate}
            \item Both populations are multivariate normal
            \item $\mathbf{\Sigma_{1} = \Sigma_{2}}$ (same co-variance matrix)
        \end{enumerate}

        Here, we are assuming several pairs of variances and co variances are nearly equal. Consequently, we can \textit{pool} the information in both samples to estimate the common co variance. 
        
        \begin{equation}
            \mathbf{S}_{pooled} = \frac{n_{1} - 1}{n_{1} + n_{2} - 2}\mathbf{S}_{1} + \frac{n_{2} - 1}{n_{1} + n_{2} - 2}\mathbf{S_{2}}
        \end{equation}
        %[item]{For $\Sigma_{1} = \Sigm_{2}$ we are assuming that several pairs of variances and co variances are nearly equal.}
    \end{frame}
    
    \begin{frame}{Assumptions Cont.}
        \begin{itemize}
            \item By the independence assumption in slide (14) implies $\mathbf{\overline{X}}_{1}$ and $\mathbf{\overline{X}}_{2}$ are independent thus $COV(\mathbf{\overline{X}_{1}, \overline{X}_{2}}) = 0$.Where 
            
            \begin{equation}
                \left(\frac{1}{n_{1}} + \frac{1}{n_{2}}\right)\mathbf{S}_{pooled}
            \end{equation}
            
            is an estimator of $COV(\mathbf{\overline{X_{1}}} - \mathbf{\overline{X}}_{2})$. The likelihood ratio test of $H_{0}: \mathbf{\mu_{1} - \mu_{2}} = \delta_{0}$ is base on $T^{2}$. Reject $H_{0}$ if, 
            
            \begin{equation}
                T^{2} = (\mathbf{\overline{x_{1}} - \overline{x_{2}} - \delta_{0}})'\left[\left(\frac{1}{n_{1}} + \frac{1}{n_{2}}\right)\mathbf{S}_{pooled}\right]^{-1}(\mathbf{\overline{x_{1}} - \overline{x_{2}} - \delta_{0}}) > c^{2}
            \end{equation}
            
            Where the \textbf{critical distance} $c^{2}$ is determined from the distribution of the two-sample $T^{2}$ statistic.
            
        %[item]{This is a test to see if is works, and it works well}
        \end{itemize}
    \end{frame}
    
    \begin{frame}{Assumptions Cont.}
        \label{res:2}
        \begin{block}{Result 6.2}
            If $\mathbf{X_{11},X_{12},...,X_{1n}}$ is a random sample of size $n_{1}$ from the $N_{p}(\mu_{1},\Sigma)$ and $\mathbf{X_{21},X_{22},...,X_{2n_{2}}}$ is an independent random sample of size $n_{2}$ from $N_{P}(\mu_{2},\Sigma)$, then 
            
            \begin{equation}
                P\left(\mathbf{\overline{X}} - \mathbf{\overline{X}} - (\mathbf{\mu_{1} - \mu_{2}}))'\left[\left(\frac{1}{n_{1}} + \frac{1}{n_{2}}\right)\right]^{-1}(\mathbf{\overline{X}_{1} - \overline{X_{2}} - (\mu_{1} - \mu_{2}})) \leq c^{2} \right] = 1 - \alpha
            \end{equation}
            
            Where, 
            
            \begin{equation*}
                c^{2} = \frac{(n_{1} + n_{2} - 2)p}{(n_{1} + n_{2} - p - 1)}F_{p,n_{1}+n_{2}-p-1}(\alpha)
            \end{equation*}
        \end{block}
        
        We are interested in the confidence regions for $\mathbf{\mu_{1} - \mu_{2}}$. From \textbf{Result 6.2} we conclude that all $\mu_{1} - \mu_{2}$ within squared statistical distance $c^{2}$ of $\mathbf{\overline{x_{1}} - \overline{x_{2}}}$ constitute the confidence region (forms an ellipse). 
        %[item]{This is a test to see if is works, and it works well}
    \end{frame}
    
    \subsection{Simultaneous Confidence intervals}
    
        \begin{frame}{Simultaneous Confidence intervals}
        \begin{itemize}
            \item Deriving simultaneous confidence intervals for the components of the vector $\mathbf{\mu_{1} - \mu_{2}}$. We assume that parent multivariate populations are normal with a common co variance $\mathbf{\Sigma}$
        \end{itemize}
        
        \begin{block}{Result 6.3}
            Let $c^{2} = (n_{1} + n_{2} - 2)p/(n_{1} + n_{2} - p - 1)F_{p,n_{1}+n_{2}-p-1}(\alpha)$ with probability $1 - \alpha$
            
            \begin{equation*}
                \mathbf{a'(\overline{X}_{1} - \overline{X}_{2}}) \pm c \sqrt{\mathbf{a}'\left(\frac{1}{n_{1}} + \frac{1}{n_{2}}\right)\mathbf{S}_{pooled}\mathbf{a} }
            \end{equation*}
            
            will cover $\mathbf{a}(\mathbf{\mu_{1} - \mu_{2}})$ for all $\mathbf{a}$. In particular $\mu_{1i} - \mu_{2i}$ will be covered by, 
            
            \begin{equation*}
                (\overline{X_{1i}} - \overline{X}_{2i}) \pm c\sqrt{\left(\frac{1}{n_{1}} + \frac{1}{n_{2}}\right)s_{ii,pooled}}
            \end{equation*}
        \end{block}
    
    \end{frame}
    
\subsubsection{Example 6.4}
    \begin{frame}{Example 6.4}
        \begin{figure}
            \includegraphics[scale=0.29]{Example 6.3 part 2.PNG}
            \includegraphics[scale=0.29]{Example 6.3.PNG}
            \caption{Constructing Simultaneous Confidence Intervals}
        \end{figure}
        
        \begin{itemize}
            \item Using Equations (10) and Result 6.3. Note, the Bonferroni $100(1 - \alpha)\%$ \textbf{simultaneous confidence intervals} for the $p$ population mean differences
            
            \begin{equation}
                \mu_{1i} - \mu_{2i}: (\overline{x}_{1i} - \overline{x}_{2i}) \pm t_{n_{1} + n_{2}-2}\left(\frac{\alpha}{2p} \right)\sqrt{\frac{1}{n_{1} + \frac{1}{n_{2}}}s_{ii,pooled}}
            \end{equation}
            
        \end{itemize}
        
        %[item]{Start by finding the 95\% confidence intervals for the differences in mean components}
        %[item]{Conclude that there is a difference in electrical consumption between those with air-conditioning and those without.}
        %[item]{where $t_{n_{1} + n_{2}}(\alpha/2p)$ is the upper $100(\alpha/2p)$th percentile of a t-distribution with $n_{1} + n_{2} - 2$ d.f.}
    \end{frame}

    
    \subsection{The Two Sample Situation when $\mathbf{\Sigma \neq \Sigma}$}
    
    \begin{frame}{The Two Sample Situation when $\mathbf{\Sigma \neq \Sigma}$}
        \begin{itemize}
            \item When $\mathbf{\Sigma \neq \Sigma}$ we are unable to find the "distance" measure like $T^{2}$, whose distribution does not depend on the unknowns $\mathbf{\Sigma_{1},\Sigma_{2}}$.
            \begin{itemize}
                \item Bartlett's test equality of $\mathbf{\Sigma_{1}},\mathbf{\Sigma_{2}}$ in terms of generalized variances. 
                \item Misleading when populations non-normal
            \end{itemize}
        \item Less Sensitive test, Tiku and Balakrishnan [23]
        \item Size of the discrepancies depend on number of $p$-variables
        \end{itemize}
        
        \begin{block}{Result 6.4}
            Let the sample sizes be such that $n_{1} - p$ and $n_{2} - p$ are large. Then the approximate $100(1 - \alpha)\%$ confidence ellipsoid (see slide 16) for $\mathbf{\mu_{1} - \mu_{2}}$ is given by all $\mathbf{\mu_{1} - \mu_{2}}$ satisfying, 
            
            \begin{equation*}
                [\overline{x}_{1} - \overline{x}_{2} - (\mathbf{\mu_{1} - \mu_{2})}]'\left[\frac{1}{n_{1}}\mathbf{S}_{1} + \frac{1}{n^{2}\mathbf{S}_{2}}\right]^{-1}[\overline{x}_{1} - \overline{x}_{2} - (\mathbf{\mu_{1} - \mu_{2})}] \leq \chi^{2}_{p}(\alpha)
            \end{equation*}
            
            \begin{equation*}
                \mathbf{a}'(\overline{x}_{1} - \overline{x}_{2}) \pm \sqrt{\chi_{p}^{2}(\alpha)}\sqrt{\mathbf{a'}\left(\frac{1}{n_{1}}\mathbf{S}_{1} + \frac{1}{n_{2}}\mathbf{S}_{2}\right)\mathbf{a}}
            \end{equation*}
        \end{block}
    \end{frame}
    
\subsection{An Approximation to the Distribution of $T^{2}$ for Normal Populations when Sample Sizes are not Large}

    \begin{frame}{Distribution of $T^{2}$}
    
    
        \begin{itemize}
            \item We can test $H_{0}: \mathbf{\mu_{1} - \mathbf{\mu_{2}}} = 0$ when population co-variance matrices are unequal even if the two sample sizes are not large provided the two populations are multivariate normal. 
            \begin{itemize}
                \item Behrens-Fisher Problem
            \end{itemize}

        
        \begin{equation}
            T^{2} = (\mathbf{\overline{X}_{1} - \overline{X}_{2} - (\mu_{1} - \mu_{2}}))'\left[\frac{1}{n_{1}\mathbf{S}_{1} + \frac{1}{n_{2}}\mathbf{S_{2}}}\right]^{-1}(\mathbf{\overline{X}_{1} - \overline{X}_{2} - (\mu_{1} - \mu_{2}}))
        \end{equation}
        
        \item recommended approximation for smaller samples is given by, 
        
        \begin{equation}
            T^{2} = \frac{\nu p}{\nu - p + 1}F_{p,\nu - p + 1}
        \end{equation}
        
        \item For normal populations, the approximation to the distribution of $T^{2}$ given by (18) generates reasonable results. 
        
        \end{itemize}
    \end{frame}

%                           6.4 START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{6.4 Comparing Several Multivariate Population Means (One-Way MANOVA)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{One-Way Manova}
    \begin{frame}{Assumptions \& Summary of ANOVA}
    
    \textbf{Assumptions about the structure of the Data for One-Way MANOVA}
        \begin{enumerate}
            \item $\mathbf{X}_{\ell 1}, X_{\ell 2}, ..., X_{\ell n_{\ell}}$ is a random sample of size $n_{\ell}$ from a population with mean $\mathbf{\mu_{\ell}}$ where $\ell = 1,2,...,g$. The random samples from different populations are independent. 
            \item All populations have a common co variance matrix $\mathbf{\Sigma}$.
            \item Each population is multivariate normal
        \end{enumerate}
    
        \begin{itemize}
            \item Summary of ANOVA 
            
            \begin{align}
                \mu_{\ell} &= \mu + \tau_{\ell}\\
                \mathbf{X}_{\ell j} &= \mu + \tau_{\ell} + e_{\ell j}\\
                x_{\ell j} &= \overline{x} + (\overline{x}_{\ell} - \overline{x}) + (x_{\ell j} - \overline{x}_{\ell})
            \end{align}
        \end{itemize}
    
    %[item]{$\mu_{\ell}$ is the sum of an overall average mean component, such as $\mu$, and a component due to the specific population.}
    %[item]{We are investigating the deviations of these populations denoted as $\tau_{\ell}$ associated with the $\ell$th population treatment}
    %[item]{Equation (16) where $\ell$th population mean, $\mu$ is the overall mean, and $\tau_{\ell}$ is the (treatment) effect.}
    %[item]{Equation (17) $X_{\ell j}$ is distributed as $N(\mu + \tau_{\ell},\sigma^{2})$}
    %[item]{(Equation (18) $(x_{\ell j} - \overline{x}_{\ell})$ is the estimated treatment effect, and $(x_{\ell j} - \overline{x}_{\ell})$ is the residual}
    
    \end{frame}
    
\subsection{MANOVA}
    
    \begin{frame}{Comparing $g$ population Mean Vectors}
        Paralleling the uni-variate reparameterization (see equation (16,17,18)) we specify the \textbf{MANOVA} model. 
        
        \begin{align}
            \mathbf{X_{\ell j} &= \mu + \tau_{\ell} + e_{\ell j} } && j = 1,2,...,n_{\ell} && \text{and} && \ell = 1,2,...,g\\
            \mathbf{x_{\ell j} &= \overline{x} + (\overline{x}_{\ell} - \overline{x}) + x_{\ell} - \overline{x}_{\ell})}
        \end{align}
        
        \begin{itemize}
            \item Decomposing Equation (20) leads t the multivariate analog of the uni-variate sum of squares breakup.
            \item Expanding $\mathbf{(x_{\ell j} - \overline{x})(x_{\ell j} - \overline{x})'}$ leads to the \textbf{within sum of squares and cross product matrix}
        \end{itemize}
        
        \begin{align}
            \mathbf{W} &= \sum_{\ell = 1}^{g} \sum_{j = 1}^{n_{\ell}}(\mathbf{x_{\ell_{j}} - \overline{x}_{\ell}) (x_{\ell j} - \overline{x}_{\ell})'} \\
            &= (n_{1} - 1)\mathbf{S}_{1} + (n_{2} - 1)\mathbf{S}_{2} + \hdots + (n_{g} - 1)\mathbf{S}_{g}
        \end{align}
    \end{frame}
    
    \renewcommand{\arraystretch}{1.75}
    \begin{frame}{Comparing $g$ population Mean Vectors Cont.}
        \begin{center}
        
            \begin{tabular}{  lm{1em}  m{5cm} m{2cm}  } 
            \hline 
            \multicolumn{3}{c}{MANOVA table for Comparing Population Mean Vectors}\\
            \hline
            Source of Variation & Matrix of SS and CP & d.f.\\
            \hline
            
            
            Treatment & \mathbf{B} = \sum_{\ell = 1}^{g} n_{\ell}(\mathbf{\overline{x}_{\ell} - \overline{x})(\overline{x}_{\ell} - \overline{x})'} & g - 1 \\ 
            Residual (Error) & $\mathbf{W}$ & \sum_{\ell = 1}^{g} n_{\ell} - g \\ 
            \hline
            Total (corrected for the mean) & $\mathbf{B + W}$ & $\sum_{\ell =1}^{g} n_{\ell} - 1$\\ 
            \hline
            
            
            \end{tabular}
        \end{center}
        %[item]{This table is exactly the same form, component by component, as the ANOVA table, except that squares of scalars are replaced by their vector counterparts.}
        %[item]{d.f. correspond to the univariate geometry and also to some multivariate distribution theory involving Wishart Densities}
    \end{frame}
    
    
    \begin{frame}{Comparing $g$ population Mean Vectors Cont.}
        \begin{itemize}
            \item One test of $H_{0}: \tau_{1} = \tau_{g} = \hdots = \tau_{g} = 0$ involves generalized variances and we reject $H_{0}$ if the ratio of generalized variances
            
            \begin{equation}
                \Lambda^{*} = \frac{|\mathbf{W}|}{\mathbf{|B + W|}}
            \end{equation}
            
            are too small. For large sample sizes, modifications to $\Lambda^{*}$, can be used to test $H_{0}$. (refer to \S{6.4}, table 6.3, pg. 303 of textbook).
            
            
        \end{itemize}
    \end{frame}

%                           6.5 START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{6.5 Simultaneous Confidence Intervals for Treatment Effects}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{6.5 Introduction}
    \begin{frame}{Simultaneous Confidence intervals for Treatment Effects} % Title of the current slide/frame 
        \begin{itemize}
            \item Let $\tau_{ki}$ be the $i$th component of $\tau_{k}$. Since $\tau_{k}$ is estimated by $\hat{\mathbf{\tau_{k}}} = \mathbf{\overline{x}_{k} - \overline{x}}$. 
            
            \begin{equation}
                \hat{\tau}_{ki} = \overline{x}_{ki} - \overline{x}_{i}
            \end{equation}
            
            \item For $p$ variables and $g(g-1)/2$ pairwise differences, so each two-sample t-interval will employ the critical value $t_{n-g}(\alpha/2m)$ where, 
            
            \begin{equation}
                m = pg(g - 1)/2
            \end{equation}
            
            is the number of simultaneous confidence statements. 
            
            %[item]{When the hypothesis of equal treatment effects is rejected the rejection of the hypothesis are of interest. The Bonferroni approach, can be used to construct simultaneous confidence intervals for the components of the differences.}
        \end{itemize}
        
        \begin{block}{Result 6.5}
            Let $n = \sum_{k=1}^{g}n_{k}$. For the the model in \textbf{(19)} with confidence at least $(1 - \alpha)$, 
            
            \begin{align*}
                \tau_{ki} - \tau_{\ell i} && \text{belongs to} && \overline{x}_{ki} - \overline{x}_{\ell i} \pm t_{n - g}\left(\frac{\alpha}{pg(g - 1)}\right)\sqrt{\frac{w_{ii}}{n - g}\left(\frac{1}{n_{k}} + \frac{1}{n_{\ell}}\right)}
            \end{align*}
            
            for all components $i = 1,...,p$ and all differences $\ell < k = 1,...,g$. Here $w_{ii}$ is the $i$th diagonal element of $\mathbf{W}$. 
        \end{block}
        
        
    \end{frame}
    


%                           6.6 START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{6.6 Testing for Equality of Covariance Matrices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{6.6 Introduction}
    \begin{frame}{Testing for Equality of Covariance Matrices} % Title of the current slide/frame 
            \begin{itemize}
                \item One assumption made when comparing two or more multivariate mean vectors is that the covariance matrices of the potentially different populations are the same. 
                \item Test the equality of the population covariance matrices. 
                \begin{itemize}
                    \item Box's M -test 
                \end{itemize}
                
                \begin{align}
                    &H_{0}: \mathbf{\Sigma_{1} = \Sigma_{2} = \hdots = \Sigma_{g} = \Sigma}\\
                    &\Lambda = \prod_{\ell}\left(\frac{|\mathbf{S}_{\ell}|}{|\mathbf{S}_{pooled}|}\right)^{(n_{\ell} - 1)/2}\\
                    &M = \left[\mathbf{\Sigma}_{\ell}(n_{\ell} -1)\right] \ln{|\mathbf{S}_{pooled}|} - \mathbf{\Sigma}[(n_{\ell - 1})\ln{|\mathbf{S}_{\ell}|}]
                \end{align}
            
            \end{itemize}
            %[item]{For $g$ populations where $\Sigma_{\ell}$ is the covariance matrix for the $\ell$th population $\ell = 1,2,...,g$ and $\Sigma$ is the presumed common covariance matrix}
            %[item]{Assuming multivariate normal populations, a \textbf{likelihood ratio statistic} for testing (29) is given by (30)}
            %[item]{Box test is based on his $\chi^{2}$ approximation to the sampling distribution $-2\ln \Lambda$}
    \end{frame}
    
    \begin{frame}{Box's Test for Equality of Covariance Matrices}
        Set, 
        
        \begin{equation}
            u = \left[\mathbf{\Sigma_{\ell}} \frac{1}{(n_{\ell} - 1)} - \frac{1}{\mathbf{\Sigma_{\ell}}(n_{\ell} - 1)}\right]\left[ \frac{2p^{2} + 3p - 1}{6(p + 1)(g - 1)} \right]
        \end{equation}
        
        then, 
        
        \begin{equation}
            C = (1 - u)M
        \end{equation}
        
        Has an approximate $\chi^{2}$ distribution with
        
        \begin{equation}
            \nu = g\frac{1}{2}p(p + 1) - \frac{1}{2}p(p + 1) = \frac{1}{2}p(p+1)(g-1)
        \end{equation}
        
        degrees of freedom at significance level $\alpha$, reject $H_{0}$ if $C > \chi^{2}_{p(p+1)(g-1)/2}(\alpha)$
        
        \begin{itemize}
            \item Box's $\chi^{2}$ approximation works well if each $n_{\ell}$ exceeds 20 and if $p$ and $g$ do not exceed 5.
        \end{itemize}
        
    \end{frame}

%                       6.7 START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{6.7 Two Way Multivariate Analysis of Variance (Fixed Effects Model)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Univariate Two-Way Fixed-Effects Model with Interaction}

    \begin{frame}{Univariate Two-Way Fixed-Effects Model with Interaction} 
    
        \begin{itemize}
            \item Suppose there are $g$ levels of factor 1 and $b$ levels of factor 2, and that $n$ independent obersevations can be observed at each of the $gb$ combination levels.
            
            \item Denote the \textbf{univariate two way model} as
             
            \begin{equation}
                X_{\ell k r} = \mu + \tau_{\ell} + \beta_{k} + \gamma_{\ell k} + e_{\ell k r}
            \end{equation}
            
            \begin{align*}
                \ell &= 1,2,\hdots,g\\
                k &= 1,2,\hdots, b\\
                r &= 1,2,\hdots,n\\
            \end{align*}
            
            Where the sums of the random variables and $e_{\ell kr}$ are independent $N(0,\sigma^{2})$ random variables.
            
            
        \end{itemize}
        
        %[item]{Let the two sets of experimental conditions be the levels of, for instance, factor 1 and factor 2, respectively.}
        %[item]{Denote the $r$th observation at level $\ell$ of factor 1 and level $k$ of factor 2 by $X_{\ell k 4}$, we specify the two-way model as}
        %[item]{Denote $\mu$ as an overall level, $\tau_{\ell}$ represents the fixed effect of factor 2, and $\gamma_{\ell k}$ is the interaction between factor 1 and factor 2. }
        %[item]{$y_{\ell k}$ implies that the factor effects are not additive and complicates the interpretation of the results.}
    \end{frame}
    
\subsection{Multivariate Two-Way Fixed Effects Model with Interaction}
    
    \begin{frame}{Multivariate Two-Way Fixed Effects Model with Interaction}
    
    \begin{itemize}
        \item The two-way fixed effects model for a \textit{vector} response consisting of $p$ components. 

    
        \begin{equation}
            \mathbf{X}_{\ell k r} = \mathbf{\mu} + \mathbf{\tau}_{\ell} + \mathbf{\beta}_{k} + \mathbf{\gamma}_{\ell k} + \mathbf{e}_{\ell k r}
        \end{equation}
            
        \begin{align*}
                \ell &= 1,2,\hdots,g\\
                k &= 1,2,\hdots, b\\
                r &= 1,2,\hdots,n\\
        \end{align*}
            
        \item The vectors are all of order $p \times 1$, and the $\mathbf{e}_{\ell k r}$ are independent $N_{p}(\mathbf{0},\mathbf{\Sigma})$ random vectors (MANOVA table for comparing Factors and their interaction in \S{6.7} page 316).     
        
        %[item]{The generalization from the univariate to the multivariate analysis consists simply of replacing a scalar such as $(\overline{x}_{\ell} - \overline{x})^{2}$ with the corresponding matrix $(\mathbf{\overline{x}} - \mathbf{\overline{x}})\mathbf{(\overline{x}_{\ell} - \overline{x}})'$}
        
        
        
    \end{itemize}
    \end{frame}

    \begin{frame}{Multivariate Two-Way Fixed Effects Model with Interaction Cont. }
        \begin{itemize}
            \item For the likelihood ratio test of $H_{0}:\gamma_{11} = \gamma_{12} = \hdots = \gamma_{gb} = \mathbf{0}$ (no interaction effects versus $H_{1}: \text{At least one } \mathbf{\gamma_{\ell k}} = \mathbf{0}$ is conducted by rejecting $H_{0}$ for small values of the ratio, 
            
            \begin{equation}
                \Lambda^{*} = \frac{|\text{SSP}_{\text{res}}|}{|\text{SSP}_{\text{int}} + \text{SSP}_{\text{res}}|}
            \end{equation}
            
            \item $p$ Univariate two-way analyses of variance (one for each variable) are often conducted to see whether the interaction appears in some responses not others. 
            
            \item Consider $H_{0}: \mathbf{\tau_{1} = \tau_{2} = \hdots = \tau_{g} = 0}$ and $H_{1}: \text{at least one } \mathbf{\tau_{\ell} \neq 0}$.
            
            \begin{equation}
                \Lambda^{*} = \frac{|\text{SSP}_{\text{res}}|}{|\text{SSP}_{\text{fac1}} + \text{SSP}_{\text{res}}|}
            \end{equation}
            
            \item For factor 2 effects, $H_{0}: \mathbf{\beta_{1}} = \mathbf{\beta_{1}} = \hdots = \mathbf{\beta_{1}} = \mathbf{0}$ and $H_{1}:$ at least one $\beta_{k} \neq \mathbf{0}$
            
            \begin{equation}
                \Lambda^{*} = \frac{|\text{SSP}_{\text{res}}|}{|\text{SSP}_{\text{fac2}} + \text{SSP}_{\text{res}}|}
            \end{equation}
            
        \end{itemize}
        %[item]{In the multivariate model we test for factor 1 and factor 2 main effects as follows}
    \end{frame}
    
    \begin{frame}{Multivariate Two-Way Fixed Effects Model with Interaction Cont.}
    
    \textbf{Reject $H_{0}: \mathbf{\gamma}_{11} = \mathbf{\gamma}_{12} = \hdots = \mathbf{\gamma}_{gb} = 0$ at level $\alpha$ if}
    
        \begin{equation}
            -\left[gb(n -1) - \frac{p + 1 - (g - 1)(b - 1)}{2}\right]\ln{\Lambda^{*} > \chi^{2}_{(g - 1)(b - 1)p(\alpha)}}
        \end{equation}
        
    \textbf{Reject $H_{0}: \mathbf{\beta}_{1} = \mathbf{\tau}_{2} = \hdots = \mathbf{\tau}_{b} = 0$ at level $\alpha$ if} (no factor 1 effects)
    
        \begin{equation}
            -\left[gb(n -1) - \frac{p + 1 - (g - 1)(b - 1)}{2}\right]\ln{\Lambda^{*} > \chi^{2}_{(g - 1)p(\alpha)}}
        \end{equation}
    
    \textbf{Reject $H_{0}: \mathbf{\beta}_{1} = \mathbf{\beta}_{2} = \hdots = \mathbf{\beta}_{b} = 0$ at level $\alpha$ if} (no factor 2 effects)
    
        \begin{equation}
            -\left[gb(n -1) - \frac{p + 1 - (g - 1)(b - 1)}{2}\right]\ln{\Lambda^{*} > \chi^{2}_{(g - 1)(b - 1)p(\alpha)}}
        \end{equation}
    \end{frame}
    
    
    \begin{frame}{Fixed Effects Model with Interaction Cont. Simultaneous Confidence Intervals}
        
        \begin{itemize}
            \item \textbf{Simultaneous confidence intervals} for contrasts in the model parameters can provide insights into the nature of the factor effects. 
            \item The \textbf{Bonferroni} approach applies to the components of the differences of the factor 1 effects and components of factor 2 effects, respectively. 
            
            \item $100(1 - \alpha)\%$ simultaneous confidence interval where $\nu = gb(n-1)$, $E_{ii}$ is the $i$th diagonal element of $\mathbf{E} = SSP_{res}$ and $\overline{x}_{\ell i} - \overline{x}_{mi}$ is the $i$th component of $\overline{\mathbf{x}}_{\ell} - \overline{\mathbf{x}}_{m}$
            
            \begin{align}
                \tau_{\ell i} - \tau_{mi} && \text{Belongs to} && (\overline{x}_{mi} - \overline{x}_{mi}) \pm t_{\nu}\left(\frac{\alpha}{pg(g - 1)}\right)\sqrt{\frac{\mathbf{E}_{ii}}{\nu}\frac{2}{bn}}
            \end{align}
            
            \item $\nu$ and $E_{ii}$ are as just defined as $\overline{x}_{ki} - \overline{x}_{qi}$ is the $i$th component of $\overline{\mathbf{x}}_{k} - \overline{\mathbf{x}}_{q}$
            
            \begin{align}
                \beta_{k i} - \beta_{qi} && \text{Belongs to} && (\overline{x}_{ki} - \overline{x}_{qi}) \pm t_{\nu}\left(\frac{\alpha}{pg(g - 1)}\right)\sqrt{\frac{\mathbf{E}_{ii}}{\nu}\frac{2}{gn}}
            \end{align}
            
        \end{itemize}
        
        
    \end{frame}

%                       6.8 START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{6.8 Profile Analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{6.8 Introduction}
    \begin{frame}{Profile Analysis Introduction} % Title of the current slide/frame 
    \begin{itemize}
        \item \textbf{Profile Analysis} pertains to situations in which a battery of $p$ treatments are administered to \textit{two or more groups of subjects}. 
        \item Construct profiles for each \textbf{population} (group).  
        \begin{itemize}
            \item $\mathbf{\mu_{1}'} =  [\mu_{11},\mu_{12},\hdots,\mu_{1p}]$ and $\mathbf{\mu}_{2}' = [\mu_{21},\mu_{22},\hdots,\mu_{2p}]$ are the \textit{mean responses} to $p$ \textit{treatments} for populations 1 and 2
        \end{itemize}
        \item $H_{0}: \mathbf{\mu_{1} = \mu_{2}}$ implies that the treatments have the \textbf{same (average) effect} on the two populations.\\

        
    \end{itemize}
    \begin{center}
        \begin{tabular}{lclc}
        \hline 
        (Stage) Question?  & Acceptable? Equivalently, \\
        \hline 
           (1) Are the Profiles \textbf{Parallel}? &  $H_{01}:\mu_{1i} - \mu_{1i-1} = \mu_{2i} - \mu_{2i - 1}$ for $i = 2,3,\hdots,p$\\
            (2) Are the Profiles \textbf{Coincident}? & $H_{02}: \mu_{1i} = \mu_{2i}$ for $i = 1,2,\hdots,p$\\
            (3) Are the Profiles \textbf{Level}? & $H_{03}: \mu_{11} = \mu_{12} = \hdots = \mu_{1p} = \mu_{21} = \mu_{22} \hdots = \mu_{2p}$
        \end{tabular}
    \end{center}

    \end{frame}
    
\subsection{Testing Profiles in Stages}
    
    \begin{frame}{Testing Profiles in Stages}
    \begin{adjustwidth}{-2em}{-2em}
    \renewcommand{\arraystretch}{2.5}
        \begin{center}
            \begin{tabular}{lccc}
            \hline
            \textbf{Stage} & \textbf{Null Hypothesis} & \textbf{Reject at level $\alpha$ if} \\ 
            \hline
                \multicolumn{3}{c}{\textcolor{red}{Test for Parallel Profiles \textbf{(0)}}}\\
                \textbf{(1)} & $H_{01}: = \mathbf{C\mu_{1}} = \mathbf{C\mu_{2}}$ & $(\overline{\mathbf{x}} - \overline{\mathbf{x}})'\mathbf{C}'[(\frac{1}{n_{1}} + \frac{1}{n_{2}})\mathbf{CS_{p}C'}]^{-1}\mathbf{C}(\mathbf{\overline{x}_{2} - \overline{x}_{2}) > c^{2}}$ \\
                \multicolumn{3}{c}{\textcolor{red}{Given Profiles are Parallel \textbf{(1)}}}\\
                \textbf{(2)} & $H_{02}:\mathbf{1'}\mu_{1} = \mathbf{1'}\mu_{2}$ & $\left(\frac{\mathbf{1}'(\mathbf{\overline{x_{1}}} - \mathbf{\overline{x_{2}}}) }{\sqrt{\left(\frac{1}{n_{1}} + \frac{1}{n_{2}}\right)} \mathbf{1'S_{p}1}}\right)^{2} > t^{2}_{n_{1} + n_{2} - 2}\left(\frac{\alpha}{2}\right) = F_{1,n_{1} + n_{2} -2}(\alpha)$ \\
                \multicolumn{3}{c}{\textcolor{red}{Given Profiles are Coincident \textbf{(2)}}}\\
                \textbf{(3)} & $H_{03}: \mathbf{C\mu} = \mathbf{0}$ & $(n_{1} + n_{2})\overline{\mathbf{x}}'\mathbf{C}'[\mathbf{CSC']^{-1}}\mathbf{C\overline{x}} > c^{2}$\\
            \hline
            \end{tabular}
        \end{center}
    \end{adjustwidth}
    \end{frame}
    
    \begin{frame}{Last Remark}
        \begin{alertblock}{Remark 1:}
            When the Sample sizes are small, a profile analysis will depend on the normality assumption. This assumption can be checked, using methods discussed in Chapter 4, with the original observations $\mathbf{x_{\ell j}}$ or the contrast observations $\mathbf{Cx_{\ell j}}$. Moreover, \textbf{analysis of several populations} proceeds in much the \textit{same fashion as that for two populations}. 
        \end{alertblock}
    \end{frame}
    
    

%                       6.9 START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{6.9 Repeated Measures Designs and Growth Curves}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{6.9 Introduction}
    \begin{frame}{Repeated Measures Introduction} % Title of the current slide/frame 
        \begin{itemize}
            \item \textbf{Repeated measures} refers to situations where the same characteristic is observed, at different times or locations, on the same subject. 
            \item The \textbf{Growth Model} measures a single treatment applied to each subject over a period of time.  
            \item Consider the following example
            \begin{itemize}
                \item \textit{Question: Can the growth pattern be adequately represented by a polynomial in time?}
            \end{itemize}
            
    \begin{figure}
        \includegraphics[scale=0.25]{table 6.5.PNG}
        \caption{\textbf{Table 6.5} Ca Measurements on the Dominant Ulna; Control Group}
    \end{figure}

        \end{itemize}
    \end{frame}

% SLIDE 1

\subsection{Theory \& Notation}

\begin{frame}{Theory \& Notation} % Title of the current slide/frame 
    \begin{itemize}
        \item When $p$ measurements on all subjects are taken at time $t_{1},t_{2},\hdots,t_{p}$ the \textbf{Potthoff-Roy} model for quadratic growth becomes, 
        \item \textbf{Assumptions:} All $\mathbf{X}_{\ell j}$ are independent and have the same covariance matrix $\mathbf{\Sigma}$. Under the \textbf{quadratic growth model} the mean vectors are
        
        \begin{displaymath}
        E[\mathbf{X}_{\ell j}] 
            =
            \begin{bmatrix}
                    \beta_{0} + \beta_{2}t_{1} + \beta_{2}t_{1}^{2} \\
                    \beta_{0} + \beta_{2}t_{2} + \beta_{2}t_{2}^{2} \\
                    \vdots \\
                    \beta_{0} + \beta_{2}t_{p} + \beta_{2}t_{p}^{2} \\
            \end{bmatrix}
            =
            \begin{bmatrix}
                1 & t_{1} & t_{1}^{2} \\
                1 & t_{2} & t_{2}^{2} \\
                \vdots & \vdots & \vdots \\
                1 & t_{p} & t_{P}^{2} \\
            \end{bmatrix}
            \begin{bmatrix}
                    \beta_{\ell 0}\\
                    \beta_{\ell 1} \\
                    \beta_{\ell 2} \\
            \end{bmatrix}
            =\mathbf{B\mathbf{\beta}_{\ell}}
        \end{displaymath}
        
        %[item]{If a $q$th order polynomial is fit to the growth data}
        
        \item Under the assumption of Multivariate normality, the \textbf{maximum likelihood estimators} of the $\mathbf{\beta}_{\ell}$ are 
        
        \begin{align}
            \mathbf{\hat{\beta}}_{\ell} = (\mathbf{B}'\mathbf{S}^{-1}_{p}\mathbf{B})^{-1}\mathbf{B}'\mathbf{S}^{-1}_{p}\mathbf{\overline{X}}_{\ell} && \text{for} && \ell = 1,2,\hdots,g
        \end{align}
        
    \end{itemize}
\end{frame}

\begin{frame}{Theory \& Notation Cont.}
    \begin{itemize}
        \item Under a $q$th order polynomial, the error sum of squares and cross products (d.f: $n_{g} - g + p - q - 1$)
        
        \begin{equation}
            \mathbf{W}_{q} = \sum_{\ell = 1}^{g}\sum_{j = 1}^{n_{\ell}}(\mathbf{X_{\ell j}} - \mathbf{B}\mathbf{\hat{\beta}_{\ell}})(\mathbf{X}_{\ell j} - \mathbf{B}\mathbf{\hat{\beta}_{\ell}})'
        \end{equation}
        
        \item The likelihood ratio test of the null hypothesis that the $q$-\textbf{order polynomial is adequate} can be based on \textit{Wilk's lambda}.
        
        \begin{equation}
            \Lambda^{*} = \frac{|\mathbf{W}|}{|\mathbf{W}_{q}}
        \end{equation}
        
        \item For \textit{large sample sizes}, the \textbf{null hypothesis} that the polynomial is adequate is \textbf{rejected} if
        
        \begin{equation}
            -\left(N - \frac{1}{2}(p - q + g)\right) \ln{\Lambda^{*}} > \chi^{2}_{(p-q-1)g}(\alpha)
        \end{equation}
        
    \end{itemize}
\end{frame}

%                   6.10 START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{6.10 Perspectives and Strategy for Analyzing Multivariate Models}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SLIDE 1

\subsection{A Strategy for the Multivariate Comparison of Treatments}

\begin{frame}{A Strategy for the Multivariate Comparison of Treatments} % Title of the current slide/frame 
    \begin{enumerate}
        \item \textbf{Try to identify Outliers:}
        \item \textbf{Perform a multivariate test of Hypothesis:}
        \item \textbf{Calculate the Bonferroni Simultaneous confidence intervals}
    \end{enumerate}
    
    \begin{alertblock}{Remark 2:}
        In some cases, differences may appear in only one of the many characteristics, and hold for only a few treatment combinations. Therefore, these few active differences may become lost among all the inactive ones. That is, the overall test may not show significance whereas a univariate test restricted to the specific active variable would detect the difference. 
    \end{alertblock}
    
\end{frame}

%                           BIBLIOGRAPHY 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Citations}

\begin{frame}{Citations}
    
    \begin{enumerate}
        \item Johnson, R. A.,  Wichern, D. W. (1992). Applied multivariate statistical analysis.Englewood Cliffs, N.J: Prentice Hall.
    \end{enumerate}
\end{frame}

%                           CODE 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}
